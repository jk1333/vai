{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0956f2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-cloud-aiplatform google-cloud-pipeline-components kfp scikit-learn mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53064beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"YOUR_PROJECT_ID\"\n",
    "EMAIL_RECIPIENTS = [ \"YOUR_EMAIL\" ]\n",
    "#Public data is stored in us region so we'll use that\n",
    "REGION=\"us-central1\"\n",
    "BUCKET_NAME = \"YOUR_BUCKET_NAME\"\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "SERVICE_ACCOUNT = \"YOUR SERVICE ACCOUNT pipeline job to access resources like Cloud Storage\"\n",
    "\n",
    "import os\n",
    "os.system(f\"gcloud config set project {PROJECT_ID}\")\n",
    "os.mkdir(\"training_package\")\n",
    "os.mkdir(\"training_package/trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6727859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set default compute engine account to owner for simple testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c2f4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chicago Taxi Trips dataset, for predicting the fare for a taxi ride based on features such as ride time, location and distance\n",
    "#Add 'mlops' dataset under bigquery with US multi region and run following command\n",
    "CREATE OR REPLACE TABLE `mlops.chicago`\n",
    "AS (\n",
    "    WITH\n",
    "      taxitrips AS (\n",
    "      SELECT\n",
    "        trip_start_timestamp,\n",
    "        trip_end_timestamp,\n",
    "        trip_seconds,\n",
    "        trip_miles,\n",
    "        payment_type,\n",
    "        pickup_longitude,\n",
    "        pickup_latitude,\n",
    "        dropoff_longitude,\n",
    "        dropoff_latitude,\n",
    "        tips,\n",
    "        tolls,\n",
    "        fare,\n",
    "        pickup_community_area,\n",
    "        dropoff_community_area,\n",
    "        company,\n",
    "        unique_key\n",
    "      FROM\n",
    "        `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "      WHERE pickup_longitude IS NOT NULL\n",
    "      AND pickup_latitude IS NOT NULL\n",
    "      AND dropoff_longitude IS NOT NULL\n",
    "      AND dropoff_latitude IS NOT NULL\n",
    "      AND trip_miles > 0\n",
    "      AND trip_seconds > 0\n",
    "      AND fare > 0\n",
    "      AND EXTRACT(YEAR FROM trip_start_timestamp) = 2019\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "      trip_start_timestamp,\n",
    "      EXTRACT(MONTH from trip_start_timestamp) as trip_month,\n",
    "      EXTRACT(DAY from trip_start_timestamp) as trip_day,\n",
    "      EXTRACT(DAYOFWEEK from trip_start_timestamp) as trip_day_of_week,\n",
    "      EXTRACT(HOUR from trip_start_timestamp) as trip_hour,\n",
    "      trip_seconds,\n",
    "      trip_miles,\n",
    "      payment_type,\n",
    "      ST_AsText(\n",
    "          ST_SnapToGrid(ST_GeogPoint(pickup_longitude, pickup_latitude), 0.1)\n",
    "      ) AS pickup_grid,\n",
    "      ST_AsText(\n",
    "          ST_SnapToGrid(ST_GeogPoint(dropoff_longitude, dropoff_latitude), 0.1)\n",
    "      ) AS dropoff_grid,\n",
    "      ST_Distance(\n",
    "          ST_GeogPoint(pickup_longitude, pickup_latitude),\n",
    "          ST_GeogPoint(dropoff_longitude, dropoff_latitude)\n",
    "      ) AS euclidean,\n",
    "      CONCAT(\n",
    "          ST_AsText(ST_SnapToGrid(ST_GeogPoint(pickup_longitude,\n",
    "              pickup_latitude), 0.1)),\n",
    "          ST_AsText(ST_SnapToGrid(ST_GeogPoint(dropoff_longitude,\n",
    "              dropoff_latitude), 0.1))\n",
    "      ) AS loc_cross,\n",
    "      IF((tips/fare >= 0.2), 1, 0) AS tip_bin,\n",
    "      tips,\n",
    "      tolls,\n",
    "      fare,\n",
    "      pickup_longitude,\n",
    "      pickup_latitude,\n",
    "      dropoff_longitude,\n",
    "      dropoff_latitude,\n",
    "      pickup_community_area,\n",
    "      dropoff_community_area,\n",
    "      company,\n",
    "      unique_key,\n",
    "      trip_end_timestamp\n",
    "    FROM\n",
    "      taxitrips\n",
    "    LIMIT 1000000\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa42e26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile training_package/trainer/task.py\n",
    "#Create task.py as training code\n",
    "# Import the libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from google.cloud import bigquery, bigquery_storage\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from google import auth\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import argparse\n",
    "import joblib\n",
    "import pickle\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# add parser arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--project-id', dest='project_id',  type=str, help='Project ID.')\n",
    "parser.add_argument('--training-dir', dest='training_dir', default=os.getenv(\"AIP_MODEL_DIR\"),\n",
    "                    type=str, help='Dir to save the data and the trained model.')\n",
    "parser.add_argument('--bq-source', dest='bq_source',  type=str, help='BigQuery data source for training data.')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# data preparation code\n",
    "BQ_QUERY = \"\"\"\n",
    "with tmp_table as (\n",
    "SELECT trip_seconds, trip_miles, fare,\n",
    "    tolls,  company,\n",
    "    pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude,\n",
    "    DATETIME(trip_start_timestamp, 'America/Chicago') trip_start_timestamp,\n",
    "    DATETIME(trip_end_timestamp, 'America/Chicago') trip_end_timestamp,\n",
    "    CASE WHEN (pickup_community_area IN (56, 64, 76)) OR (dropoff_community_area IN (56, 64, 76)) THEN 1 else 0 END is_airport,\n",
    "FROM `{}`\n",
    "WHERE\n",
    "  dropoff_latitude IS NOT NULL and\n",
    "  dropoff_longitude IS NOT NULL and\n",
    "  pickup_latitude IS NOT NULL and\n",
    "  pickup_longitude IS NOT NULL and\n",
    "  fare > 0 and\n",
    "  trip_miles > 0\n",
    "  and MOD(ABS(FARM_FINGERPRINT(unique_key)), 100) between 0 and 99\n",
    "ORDER BY RAND()\n",
    "LIMIT 10000)\n",
    "SELECT *,\n",
    "    EXTRACT(YEAR FROM trip_start_timestamp) trip_start_year,\n",
    "    EXTRACT(MONTH FROM trip_start_timestamp) trip_start_month,\n",
    "    EXTRACT(DAY FROM trip_start_timestamp) trip_start_day,\n",
    "    EXTRACT(HOUR FROM trip_start_timestamp) trip_start_hour,\n",
    "    FORMAT_DATE('%a', DATE(trip_start_timestamp)) trip_start_day_of_week\n",
    "FROM tmp_table\n",
    "\"\"\".format(args.bq_source)\n",
    "# Get default credentials\n",
    "credentials, project = auth.default()\n",
    "bqclient = bigquery.Client(credentials=credentials, project=args.project_id)\n",
    "bqstorageclient = bigquery_storage.BigQueryReadClient(credentials=credentials)\n",
    "df = (\n",
    "    bqclient.query(BQ_QUERY)\n",
    "    .result()\n",
    "    .to_dataframe(bqstorage_client=bqstorageclient)\n",
    ")\n",
    "# Add 'N/A' for missing 'Company'\n",
    "df.fillna(value={'company':'N/A','tolls':0}, inplace=True)\n",
    "# Drop rows containing null data.\n",
    "df.dropna(how='any', axis='rows', inplace=True)\n",
    "# Pickup and dropoff locations distance\n",
    "df['abs_distance'] = (np.hypot(df['dropoff_latitude']-df['pickup_latitude'], df['dropoff_longitude']-df['pickup_longitude']))*100\n",
    "\n",
    "# Remove extremes, outliers\n",
    "possible_outliers_cols = ['trip_seconds', 'trip_miles', 'fare', 'abs_distance']\n",
    "df=df[(np.abs(stats.zscore(df[possible_outliers_cols].astype(float))) < 3).all(axis=1)].copy()\n",
    "# Reduce location accuracy\n",
    "df=df.round({'pickup_latitude': 3, 'pickup_longitude': 3, 'dropoff_latitude':3, 'dropoff_longitude':3})\n",
    "\n",
    "# Drop the timestamp col\n",
    "X=df.drop(['trip_start_timestamp', 'trip_end_timestamp'],axis=1)\n",
    "\n",
    "# Split the data into train and test\n",
    "X_train, X_test = train_test_split(X, test_size=0.10, random_state=123)\n",
    "\n",
    "## Format the data for batch predictions\n",
    "# select string cols\n",
    "string_cols = X_test.select_dtypes(include='object').columns\n",
    "# Add quotes around string fields\n",
    "X_test[string_cols] = X_test[string_cols].apply(lambda x: '\\\"' + x + '\\\"')\n",
    "# Add quotes around column names\n",
    "X_test.columns = ['\\\"' + col + '\\\"' for col in X_test.columns]\n",
    "# Save DataFrame to csv\n",
    "X_test.to_csv(os.path.join(args.training_dir,\"test.csv\"),index=False,quoting=csv.QUOTE_NONE, escapechar=' ')\n",
    "# Save test data without the target for batch predictions\n",
    "X_test.drop('\\\"fare\\\"',axis=1,inplace=True)\n",
    "X_test.to_csv(os.path.join(args.training_dir,\"test_no_target.csv\"),index=False,quoting=csv.QUOTE_NONE, escapechar=' ')\n",
    "\n",
    "# Separate the target column\n",
    "y_train=X_train.pop('fare')\n",
    "# Get the column indexes\n",
    "col_index_dict = {col: idx for idx, col in enumerate(X_train.columns)}\n",
    "# Create a column transformer pipeline\n",
    "ct_pipe = ColumnTransformer(transformers=[\n",
    "    ('hourly_cat', OneHotEncoder(categories=[range(0,24)], sparse = False), [col_index_dict['trip_start_hour']]),\n",
    "    ('dow', OneHotEncoder(categories=[['Mon', 'Tue', 'Sun', 'Wed', 'Sat', 'Fri', 'Thu']], sparse = False), [col_index_dict['trip_start_day_of_week']]),\n",
    "    ('std_scaler', StandardScaler(), [\n",
    "        col_index_dict['trip_start_year'],\n",
    "        col_index_dict['abs_distance'],\n",
    "        col_index_dict['pickup_longitude'],\n",
    "        col_index_dict['pickup_latitude'],\n",
    "        col_index_dict['dropoff_longitude'],\n",
    "        col_index_dict['dropoff_latitude'],\n",
    "        col_index_dict['trip_miles'],\n",
    "        col_index_dict['trip_seconds']])\n",
    "])\n",
    "# Add the random-forest estimator to the pipeline\n",
    "rfr_pipe = Pipeline([\n",
    "    ('ct', ct_pipe),\n",
    "    ('forest_reg', RandomForestRegressor(\n",
    "        n_estimators = 20,\n",
    "        max_features = 1.0,\n",
    "        n_jobs = -1,\n",
    "        random_state = 3,\n",
    "        max_depth=None,\n",
    "        max_leaf_nodes=None,\n",
    "    ))\n",
    "])\n",
    "\n",
    "# train the model\n",
    "rfr_score = cross_val_score(rfr_pipe, X_train, y_train, scoring = 'neg_mean_squared_error', cv = 5)\n",
    "rfr_rmse = np.sqrt(-rfr_score)\n",
    "print (\"Crossvalidation RMSE:\",rfr_rmse.mean())\n",
    "final_model=rfr_pipe.fit(X_train, y_train)\n",
    "# Save the model pipeline\n",
    "with open(os.path.join(args.training_dir,\"model.joblib\"), 'wb') as model_file:\n",
    "    pickle.dump(final_model, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f65e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile training_package/setup.py\n",
    "#Definition of training package\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES=[\"google-cloud-bigquery[pandas]\",\"google-cloud-bigquery-storage\"]\n",
    "setup(\n",
    "    name='trainer',\n",
    "    version='0.1',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='Training application package for chicago taxi trip fare prediction.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabf86bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd training_package && python setup.py sdist --formats=gztar && cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6210faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create staging bucket\n",
    "os.system(f\"gcloud storage buckets create {BUCKET_URI} --location={REGION} --project={PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d939d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the training package to the bucket\n",
    "os.system(f\"gcloud storage cp training_package/dist/trainer-0.1.tar.gz {BUCKET_URI}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0cc139",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ROOT = \"{}/pipeline_root/chicago-taxi-pipe\".format(BUCKET_URI)\n",
    "PIPELINE_NAME = \"vertex-pipeline-datatrigger-tutorial\"\n",
    "WORKING_DIR = f\"{PIPELINE_ROOT}/mlops-datatrigger-tutorial\"\n",
    "os.environ['AIP_MODEL_DIR'] = WORKING_DIR\n",
    "EXPERIMENT_NAME = PIPELINE_NAME + \"-experiment\"\n",
    "PIPELINE_FILE = PIPELINE_NAME + \".yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011a88b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    staging_bucket=BUCKET_URI,\n",
    "    location=REGION,\n",
    "    experiment=EXPERIMENT_NAME)\n",
    "\n",
    "aiplatform.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cc9ca3",
   "metadata": {},
   "source": [
    "#Refer following link for local testing\n",
    "https://docs.cloud.google.com/vertex-ai/docs/pipelines/build-pipeline#test_a_pipeline_locally_optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1bd86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp.dsl import importer\n",
    "from kfp.dsl import OneOf\n",
    "from google_cloud_pipeline_components.v1.custom_job import CustomTrainingJobOp\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "from google_cloud_pipeline_components.v1.batch_predict_job import ModelBatchPredictOp\n",
    "from google_cloud_pipeline_components.v1.model_evaluation import ModelEvaluationRegressionOp\n",
    "from google_cloud_pipeline_components.v1.vertex_notification_email import VertexNotificationEmailOp\n",
    "from google_cloud_pipeline_components.v1.endpoint import ModelDeployOp\n",
    "from google_cloud_pipeline_components.v1.endpoint import EndpointCreateOp\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# define the train-deploy pipeline\n",
    "@dsl.pipeline(name=\"custom-model-training-evaluation-pipeline\")\n",
    "def custom_model_training_evaluation_pipeline(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    training_job_display_name: str,\n",
    "    worker_pool_specs: list,\n",
    "    base_output_dir: str,\n",
    "    prediction_container_uri: str,\n",
    "    model_display_name: str,\n",
    "    batch_prediction_job_display_name: str,\n",
    "    target_field_name: str,\n",
    "    test_data_gcs_uri: list,\n",
    "    ground_truth_gcs_source: list,\n",
    "    batch_predictions_gcs_prefix: str,\n",
    "    batch_predictions_input_format: str=\"csv\",\n",
    "    batch_predictions_output_format: str=\"jsonl\",\n",
    "    ground_truth_format: str=\"csv\",\n",
    "    parent_model_resource_name: str=None,\n",
    "    parent_model_artifact_uri: str=None,\n",
    "    existing_model: bool=False\n",
    "\n",
    "):\n",
    "    # Notification task\n",
    "    notify_task = VertexNotificationEmailOp(\n",
    "                    recipients= EMAIL_RECIPIENTS\n",
    "                    )\n",
    "    with dsl.ExitHandler(notify_task, name='MLOps Continuous Training Pipeline'):\n",
    "        # Train the model\n",
    "        custom_job_task = CustomTrainingJobOp(\n",
    "                                    project=project,\n",
    "                                    display_name=training_job_display_name,\n",
    "                                    worker_pool_specs=worker_pool_specs,\n",
    "                                    base_output_directory=base_output_dir,\n",
    "                                    location=location\n",
    "                            )\n",
    "\n",
    "        # Import the unmanaged model\n",
    "        import_unmanaged_model_task = importer(\n",
    "                                        artifact_uri=base_output_dir,\n",
    "                                        artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "                                        metadata={\n",
    "                                            \"containerSpec\": {\n",
    "                                                \"imageUri\": prediction_container_uri,\n",
    "                                            },\n",
    "                                        },\n",
    "                                    ).after(custom_job_task)\n",
    "\n",
    "        with dsl.If(existing_model == True):\n",
    "            # Import the parent model to upload as a version\n",
    "            import_registry_model_task = importer(\n",
    "                                        artifact_uri=parent_model_artifact_uri,\n",
    "                                        artifact_class=artifact_types.VertexModel,\n",
    "                                        metadata={\n",
    "                                            \"resourceName\": parent_model_resource_name\n",
    "                                        },\n",
    "                                    ).after(import_unmanaged_model_task)\n",
    "            # Upload the model as a version\n",
    "            model_version_upload_op = ModelUploadOp(\n",
    "                                    project=project,\n",
    "                                    location=location,\n",
    "                                    display_name=model_display_name,\n",
    "                                    parent_model=import_registry_model_task.outputs[\"artifact\"],\n",
    "                                    unmanaged_container_model=import_unmanaged_model_task.outputs[\"artifact\"],\n",
    "                                )\n",
    "\n",
    "        with dsl.Else():\n",
    "            # Upload the model\n",
    "            model_upload_op = ModelUploadOp(\n",
    "                                    project=project,\n",
    "                                    location=location,\n",
    "                                    display_name=model_display_name,\n",
    "                                    unmanaged_container_model=import_unmanaged_model_task.outputs[\"artifact\"],\n",
    "                                )\n",
    "        # Get the model (or model version)\n",
    "        model_resource = OneOf(model_version_upload_op.outputs[\"model\"], model_upload_op.outputs[\"model\"])\n",
    "\n",
    "        # Batch prediction\n",
    "        batch_predict_task = ModelBatchPredictOp(\n",
    "                            project= project,\n",
    "                            job_display_name= batch_prediction_job_display_name,\n",
    "                            model= model_resource,\n",
    "                            location= location,\n",
    "                            instances_format= batch_predictions_input_format,\n",
    "                            predictions_format= batch_predictions_output_format,\n",
    "                            gcs_source_uris= test_data_gcs_uri,\n",
    "                            gcs_destination_output_uri_prefix= batch_predictions_gcs_prefix,\n",
    "                            machine_type= 'n1-standard-2'\n",
    "                            )\n",
    "        # Evaluation task\n",
    "        evaluation_task = ModelEvaluationRegressionOp(\n",
    "                            project= project,\n",
    "                            target_field_name= target_field_name,\n",
    "                            location= location,\n",
    "                            # model= model_resource,\n",
    "                            predictions_format= batch_predictions_output_format,\n",
    "                            predictions_gcs_source= batch_predict_task.outputs[\"gcs_output_directory\"],\n",
    "                            ground_truth_format= ground_truth_format,\n",
    "                            ground_truth_gcs_source= ground_truth_gcs_source,\n",
    "                            prediction_score_column=\"fare\"\n",
    "                            )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac6a66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp import compiler\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=custom_model_training_evaluation_pipeline,\n",
    "    package_path=\"{}.yaml\".format(PIPELINE_NAME),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c968c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_NAME = \"mlops\"\n",
    "# Create a repo in the artifact registry\n",
    "os.system(f\"gcloud artifacts repositories create {REPO_NAME} --location={REGION} --repository-format=KFP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee18522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.registry import RegistryClient\n",
    "\n",
    "host = f\"https://{REGION}-kfp.pkg.dev/{PROJECT_ID}/{REPO_NAME}\"\n",
    "client = RegistryClient(host=host)\n",
    "TEMPLATE_NAME, VERSION_NAME = client.upload_pipeline(\n",
    "file_name=PIPELINE_FILE,\n",
    "tags=[\"v1\", \"latest\"],\n",
    "extra_headers={\"description\":\"This is an example pipeline template.\"})\n",
    "TEMPLATE_URI = f\"https://{REGION}-kfp.pkg.dev/{PROJECT_ID}/{REPO_NAME}/{TEMPLATE_NAME}/latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e61f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"mlops\"\n",
    "TABLE_NAME = \"chicago\"\n",
    "\n",
    "worker_pool_specs = [{\n",
    "                        \"machine_spec\": {\"machine_type\": \"e2-highmem-2\"},\n",
    "                        \"replica_count\": 1,\n",
    "                        \"python_package_spec\":{\n",
    "                                \"executor_image_uri\": \"us-docker.pkg.dev/vertex-ai/training/sklearn-cpu.1-0:latest\",\n",
    "                                \"package_uris\": [f\"{BUCKET_URI}/trainer-0.1.tar.gz\"],\n",
    "                                \"python_module\": \"trainer.task\",\n",
    "                                \"args\":[\"--project-id\",PROJECT_ID, \"--training-dir\",f\"/gcs/{BUCKET_NAME}\",\"--bq-source\",f\"{PROJECT_ID}.{DATASET_NAME}.{TABLE_NAME}\"]\n",
    "                        },\n",
    "}]\n",
    "\n",
    "parameters = {\n",
    "    \"project\": PROJECT_ID,\n",
    "    \"location\": REGION,\n",
    "    \"training_job_display_name\": \"taxifare-prediction-training-job\",\n",
    "    \"worker_pool_specs\": worker_pool_specs,\n",
    "    \"base_output_dir\": BUCKET_URI,\n",
    "    \"prediction_container_uri\": \"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\",\n",
    "    \"model_display_name\": \"taxifare-prediction-model\",\n",
    "    \"batch_prediction_job_display_name\": \"taxifare-prediction-batch-job\",\n",
    "    \"target_field_name\": \"fare\",\n",
    "    \"test_data_gcs_uri\": [f\"{BUCKET_URI}/test_no_target.csv\"],\n",
    "    \"ground_truth_gcs_source\": [f\"{BUCKET_URI}/test.csv\"],\n",
    "    \"batch_predictions_gcs_prefix\": f\"{BUCKET_URI}/batch_predict_output\",\n",
    "    \"existing_model\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14fbc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manual run of pipeline\n",
    "\n",
    "# Create a pipeline job\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"triggered_custom_regression_evaluation\",\n",
    "    template_path=TEMPLATE_URI ,\n",
    "    parameter_values=parameters,\n",
    "    pipeline_root=BUCKET_URI,\n",
    "    enable_caching=False\n",
    ")\n",
    "# Run the pipeline job\n",
    "job.run(\n",
    "    service_account=SERVICE_ACCOUNT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e323f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add job to scheduler for automated running\n",
    "job_schedule = job.create_schedule(\n",
    "  display_name=\"mlops tutorial schedule\",\n",
    "  cron=\"0 0 1 * *\", #\n",
    "  max_concurrent_run_count=1,\n",
    "  max_run_count=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed8f797",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run pipeline when there is new data\n",
    "https://cloud.google.com/vertex-ai/docs/pipelines/continuous-training-tutorial#run_the_pipeline_when_there_is_new_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
